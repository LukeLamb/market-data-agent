# Logstash Configuration for Market Data Agent
# Phase 4 Step 2: Enterprise Monitoring & Observability

input {
  # Beats input for log collection
  beats {
    port => 5044
  }

  # Syslog input for system logs
  syslog {
    port => 5514
  }

  # HTTP input for application logs
  http {
    port => 8080
    codec => json
    additional_codecs => {
      "application/json" => "json"
    }
  }

  # Kafka input for high-volume logs
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["market-data-logs", "application-logs", "audit-logs"]
    group_id => "logstash-market-data"
    consumer_threads => 3
    codec => json
  }

  # File input for local development
  file {
    path => "/var/log/market-data-agent/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => multiline {
      pattern => "^\d{4}-\d{2}-\d{2}"
      negate => true
      what => "previous"
    }
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }

  # Parse application logs
  if [fields][service] == "market-data-agent" {
    # Extract timestamp
    if [timestamp] {
      date {
        match => [ "timestamp", "ISO8601" ]
      }
    }

    # Parse log level
    if [level] {
      mutate {
        uppercase => [ "level" ]
      }
    }

    # Extract market data specific fields
    if [symbol] {
      mutate {
        add_field => { "market_data_symbol" => "%{symbol}" }
      }
    }

    if [source] {
      mutate {
        add_field => { "data_source" => "%{source}" }
      }
    }

    if [price] {
      mutate {
        convert => { "price" => "float" }
        add_field => { "market_data_price" => "%{price}" }
      }
    }

    # Parse error information
    if [error] {
      mutate {
        add_field => { "error_type" => "%{[error][type]}" }
        add_field => { "error_message" => "%{[error][message]}" }
      }
    }

    # Extract performance metrics
    if [response_time] {
      mutate {
        convert => { "response_time" => "float" }
      }
    }

    if [duration] {
      mutate {
        convert => { "duration" => "float" }
      }
    }
  }

  # Parse Kubernetes logs
  if [kubernetes] {
    # Add Kubernetes metadata
    mutate {
      add_field => { "k8s_namespace" => "%{[kubernetes][namespace_name]}" }
      add_field => { "k8s_pod" => "%{[kubernetes][pod_name]}" }
      add_field => { "k8s_container" => "%{[kubernetes][container_name]}" }
    }

    # Parse container logs
    if [log] {
      # Try to parse as JSON first
      json {
        source => "log"
        target => "parsed_log"
        skip_on_invalid_json => true
      }

      # If JSON parsing succeeded, merge fields
      if [parsed_log] {
        ruby {
          code => "
            parsed = event.get('parsed_log')
            if parsed.is_a?(Hash)
              parsed.each { |k, v| event.set(k, v) }
            end
            event.remove('parsed_log')
          "
        }
      }
    }
  }

  # Parse access logs (nginx/API gateway)
  if [fields][logtype] == "access" {
    grok {
      match => {
        "message" => "%{COMBINEDAPACHELOG}"
      }
    }

    # Convert response time to float
    if [response] {
      mutate {
        convert => { "response" => "integer" }
      }
    }

    # Parse response time from custom field
    if [request_time] {
      mutate {
        convert => { "request_time" => "float" }
      }
    }

    # Extract API endpoint
    if [request] {
      grok {
        match => {
          "request" => "%{WORD:http_method} %{URIPATH:api_endpoint}.*"
        }
      }
    }

    # Categorize response codes
    if [response] {
      if [response] >= 200 and [response] < 300 {
        mutate { add_field => { "response_category" => "success" } }
      } else if [response] >= 300 and [response] < 400 {
        mutate { add_field => { "response_category" => "redirect" } }
      } else if [response] >= 400 and [response] < 500 {
        mutate { add_field => { "response_category" => "client_error" } }
      } else if [response] >= 500 {
        mutate { add_field => { "response_category" => "server_error" } }
      }
    }
  }

  # Parse audit logs
  if [fields][logtype] == "audit" {
    # Add audit-specific fields
    mutate {
      add_field => { "audit_action" => "%{action}" }
      add_field => { "audit_user" => "%{user}" }
      add_field => { "audit_resource" => "%{resource}" }
    }

    # Parse audit timestamp
    if [audit_timestamp] {
      date {
        match => [ "audit_timestamp", "ISO8601" ]
        target => "@timestamp"
      }
    }
  }

  # Parse security logs
  if [fields][logtype] == "security" {
    # Add security event classification
    if [event_type] {
      mutate {
        add_field => { "security_event_type" => "%{event_type}" }
      }
    }

    # Parse IP addresses
    if [client_ip] {
      geoip {
        source => "client_ip"
        target => "geoip"
      }
    }
  }

  # Parse performance logs
  if [fields][logtype] == "performance" {
    # Convert numeric fields
    if [cpu_usage] {
      mutate {
        convert => { "cpu_usage" => "float" }
      }
    }

    if [memory_usage] {
      mutate {
        convert => { "memory_usage" => "integer" }
      }
    }

    if [disk_usage] {
      mutate {
        convert => { "disk_usage" => "float" }
      }
    }
  }

  # Add common fields
  mutate {
    add_field => { "[@metadata][index_prefix]" => "market-data" }
    add_field => { "ingestion_timestamp" => "%{@timestamp}" }
  }

  # Remove unwanted fields
  mutate {
    remove_field => [ "agent", "ecs", "host", "input", "tags" ]
  }

  # Add environment information
  if ![environment] {
    if [k8s_namespace] =~ /.*prod.*/ {
      mutate { add_field => { "environment" => "production" } }
    } else if [k8s_namespace] =~ /.*staging.*/ {
      mutate { add_field => { "environment" => "staging" } }
    } else if [k8s_namespace] =~ /.*dev.*/ {
      mutate { add_field => { "environment" => "development" } }
    } else {
      mutate { add_field => { "environment" => "unknown" } }
    }
  }
}

output {
  # Main elasticsearch output
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_prefix]}-%{environment}-%{+YYYY.MM.dd}"
    template_name => "market-data-logs"
    template_pattern => "market-data-*"
    template => "/usr/share/logstash/templates/market-data-template.json"
    template_overwrite => true

    # Document type based on log type
    document_type => "%{[fields][logtype]}"

    # Performance settings
    workers => 3
    flush_size => 1000
    idle_flush_time => 10
  }

  # Error logs to separate index
  if [level] == "ERROR" or [response] >= 500 {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "market-data-errors-%{environment}-%{+YYYY.MM.dd}"
    }
  }

  # Audit logs to separate index
  if [fields][logtype] == "audit" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "market-data-audit-%{environment}-%{+YYYY.MM.dd}"
    }
  }

  # Security logs to separate index
  if [fields][logtype] == "security" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "market-data-security-%{environment}-%{+YYYY.MM.dd}"
    }
  }

  # Debug output for development
  if [environment] == "development" {
    stdout {
      codec => rubydebug
    }
  }

  # Dead letter queue for failed documents
  if "_grokparsefailure" in [tags] or "_jsonparsefailure" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "market-data-failed-%{+YYYY.MM.dd}"
    }
  }
}